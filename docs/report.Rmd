---
title: "Focus marking in Autistic and Neurotypical speakers"
output: html_document
date: '2022-11-16'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(here)
library(tidyverse)
library(sjPlot)
library(bayestestR)
library(lme4)
library(brms)
mod = readRDS(here("models", "model_b.rds"))
plot_df = conditional_effects(mod)[["PROMPT_TYPE:SPEAKER_TYPE"]]

fixef_df = fixef(mod) %>% 
  as.data.frame() %>% 
  rownames_to_column("parameter") %>% 
  mutate(parameter = paste0("b_", parameter))
```

## Updates Feb 8th 2024 

I re-ran the analysis with the new data, and only the `SUBJ` condition was altered (the difference was less).

## Overall Summary 

Overall, it looks like the participants (n = 43) were more correct at identifying the referent in object questions with neurotypical speakers, but, I think surprisingly were actually better at identifying referents in subject questions by Autistic speakers.
They performed below chance in for both NT and AUT speakers in the contrastive focus condition. 

Below, I have written the statistical analysis and parts of a results section that I have tried to write to be more or less paper-ready (where I am more informal here).


## Statistical Analysis 

Since the outcome variable is binary, we ran a Bayesian logistic regression model in which the outcome was the probability of a correct response. 
The fixed effect predictors included speaker type (2 levels, neurotypical and autistic) and prompt type (3 levels, contrastive focus, subject question and object question), with a random intercepts for participant and video scene. 
<!--- Could include ints for token and model speaker, if we wanted, but this caused some issues with the ESS of the model.  ---->
The model included the default brms priors - Student’s T distribution with 3 degrees of freedom. The model was run using with 4000 iterations of Hamiltonian Monte-Carlo sampling (1000 warm up), across 4 chains and 8 processing cores.

### The Percentage of Correct Responses for autistic and neurotypical speakers for each sentence type 

Figure 1 shows the percentage of correct responses for each prompt and speaker type. 

```{r}
library(here)
knitr::include_graphics(here("docs", "img", "desc_pct.png"))
```

### The Probability of a correct response as a function of group and condition

Figure Y shows the probability of a correct responses for each prompt and speaker type.
These estimates were derived from the Bayesian Model using the conditional effects function in R (Bürkner, 2017).
The model follows the trends of the descriptive data (including all estimates to be thorough):

The probability of a correct response in object questions from NT speakers was `r round(plot_df$estimate__[4], digits = 2)` 
[HDI = `r round(plot_df$lower__[4], digits = 2)` -
`r round(plot_df$upper__[4], digits = 2)`].

The probability of a correct response in object questions from AUT speakers was  `r round(plot_df$estimate__[3], digits = 2)` 
[HDI = `r round(plot_df$lower__[3], digits = 2)` -
`r round(plot_df$upper__[3], digits = 2)`].

The probability of a correct response in subject questions from NT speakers was  `r round(plot_df$estimate__[6], digits = 2)` 
[HDI = `r round(plot_df$lower__[6], digits = 2)` -
`r round(plot_df$upper__[6], digits = 2)`].

The probability of a correct response in subject questions from AUT speakers was  `r round(plot_df$estimate__[5], digits = 2)` 
[HDI = `r round(plot_df$lower__[5], digits = 2)` -
`r round(plot_df$upper__[5], digits = 2)`].

The probability of a correct response in contrastive focus from NT speakers was `r round(plot_df$estimate__[2], digits = 2)` 
[HDI = `r round(plot_df$lower__[2], digits = 2)` -
`r round(plot_df$upper__[2], digits = 2)`],

The probability of a correct response in contrastive focus from AUT speakers was  `r round(plot_df$estimate__[1], digits = 2)` 
[HDI = `r round(plot_df$lower__[1], digits = 2)` -
`r round(plot_df$upper__[1], digits = 2)`],

```{r}
knitr::include_graphics(here("docs", "img", "mod_plot.png"))
```


### Appendix 

Something to note here and possibly include in the body of the paper is our bottom table, the pd (probability of direction) and ps (probability of significance) columns in particular. 
In each case we see very high probabilities in both cases in all fixed effects and interactions. 
I take this as good evidence that the effects are in the direction the data says they are and that these differences are not due to noise. 

```{r}
knitr::include_graphics(here("docs",                                                        "img", "mcmc_plot.png"))
```

```{r}
tab_model(mod)
```

```{r}
describe_posterior(
  mod,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
``` 

## References 

Bürkner, P. (2017). “brms: An R Package for Bayesian Multilevel Models Using Stan.” *Journal of Statistical Software*, 80(1), 1–28. doi:10.18637/jss.v080.i01.